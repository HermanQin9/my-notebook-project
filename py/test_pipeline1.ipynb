{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191d093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RUNNING IN FULL PIPELINE MODE (10000 samples)\n",
      "==================================================\n",
      "Running pipeline in FULL mode...\n",
      "Target samples: 10000\n",
      "Cache file: mp_data_cache_20k.pkl\n",
      "--------------------------------------------------\n",
      "==================================================\n",
      "Starting ML Pipeline (FULL mode)\n",
      "==================================================\n",
      "[N0] Load cache: mp_data_cache_20k.pkl\n",
      "Loaded 10000 entries from cache\n",
      "Non-Fe materials: 10000\n",
      "Fe-containing materials: 0\n",
      "\n",
      "==============================\n",
      "Feature Matrix Generation\n",
      "==============================\n",
      "Dropping 0 columns with >50.0% missing values\n",
      "Using 140 feature columns\n",
      "Train/Val/Test split: 8000/2000/0\n",
      "Training samples: 8000\n",
      "Validation samples: 2000\n",
      "\n",
      "==============================\n",
      "Imputation\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\impute\\_base.py:558: UserWarning: Skipping features without any observed values: ['crystal_system']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\impute\\_base.py:558: UserWarning: Skipping features without any observed values: ['crystal_system']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Feature Selection\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "Scaling\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "Model Training\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "Results\n",
      "==============================\n",
      "Validation MAE: 0.174\n",
      "Validation R²: 0.883\n",
      "\n",
      "Total pipeline time: 23.6s\n",
      "\n",
      "==================================================\n",
      "PIPELINE COMPLETED SUCCESSFULLY!\n",
      "==================================================\n",
      "Mode: FULL\n",
      "Data sizes - Train: 8000, Val: 2000, Test: 0\n",
      "Performance - Val MAE: 0.174, Val R²: 0.883\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from matminer.featurizers.structure import DensityFeatures, GlobalSymmetryFeatures\n",
    "from matminer.featurizers.composition import ElementProperty\n",
    "from mp_api.client import MPRester\n",
    "\n",
    "API_KEY = \"f3qtz1d2EV47QfPtknWcFSOXTaUCzNli\" \n",
    "TARGET_PROP = \"formation_energy_per_atom\"\n",
    "\n",
    "# Configuration for testing vs full pipeline\n",
    "TESTING = False  # Set to False for full pipeline\n",
    "\n",
    "if TESTING:\n",
    "    CACHE_FILE = \"mp_data_cache_1k_test.pkl\"\n",
    "    BATCH_SIZE = 20\n",
    "    N_TOTAL = 200\n",
    "    print(\"=\" * 50)\n",
    "    print(\"RUNNING IN TESTING MODE (200 samples)\")\n",
    "    print(\"=\" * 50)\n",
    "else:\n",
    "    CACHE_FILE = \"mp_data_cache_20k.pkl\"\n",
    "    BATCH_SIZE = 100\n",
    "    N_TOTAL = 10000\n",
    "    print(\"=\" * 50)\n",
    "    print(\"RUNNING IN FULL PIPELINE MODE (10000 samples)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# ================== Node类 ===================\n",
    "class Node:\n",
    "    def __init__(self, name, methods):\n",
    "        self.name = name\n",
    "        self.methods = methods\n",
    "    \n",
    "    def execute(self, method, params, data):\n",
    "        return self.methods[method](data, **params)\n",
    "\n",
    "# N0: Data Fetch & Feature工程（分批+进度条+缓存+API兼容性）\n",
    "def fetch_and_featurize(data=None, cache=True):\n",
    "    if cache and os.path.exists(CACHE_FILE):\n",
    "        print(f\"[N0] Load cache: {CACHE_FILE}\")\n",
    "        with open(CACHE_FILE, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "        print(f\"Loaded {len(df)} entries from cache\")\n",
    "    else:\n",
    "        print(f\"[N0] Fetching + feature engineering from MP API ({N_TOTAL} entries)...\")\n",
    "        elem_feat = ElementProperty.from_preset(\"magpie\")\n",
    "        dens_feat = DensityFeatures()\n",
    "        sym_feat = GlobalSymmetryFeatures()\n",
    "        \n",
    "        dfs = []\n",
    "        fetched = 0\n",
    "        \n",
    "        with MPRester(API_KEY) as mpr:\n",
    "            try:\n",
    "                # Use the search method with proper pagination\n",
    "                docs = mpr.materials.summary.search(\n",
    "                    fields=[\"material_id\", \"structure\", \"elements\", \"formula_pretty\", TARGET_PROP],\n",
    "                    chunk_size=BATCH_SIZE,\n",
    "                    num_chunks=N_TOTAL // BATCH_SIZE + 1\n",
    "                )\n",
    "                \n",
    "                # Process in batches\n",
    "                batch_count = 0\n",
    "                for batch_docs in tqdm(docs, desc=\"Processing batches\", ncols=100):\n",
    "                    if not isinstance(batch_docs, list):\n",
    "                        batch_docs = [batch_docs]\n",
    "                    \n",
    "                    # Filter out docs without target property\n",
    "                    valid_docs = [d for d in batch_docs if getattr(d, TARGET_PROP, None) is not None]\n",
    "                    \n",
    "                    if not valid_docs:\n",
    "                        continue\n",
    "                    \n",
    "                    # Create dataframe from batch\n",
    "                    df_batch = pd.DataFrame([\n",
    "                        {\n",
    "                            \"material_id\": d.material_id,\n",
    "                            \"structure\": d.structure,\n",
    "                            \"elements\": d.elements,\n",
    "                            \"formula_pretty\": getattr(d, \"formula_pretty\", None),\n",
    "                            TARGET_PROP: getattr(d, TARGET_PROP, None)\n",
    "                        }\n",
    "                        for d in valid_docs\n",
    "                    ])\n",
    "                    \n",
    "                    # Remove entries without structure\n",
    "                    df_batch = df_batch.dropna(subset=[\"structure\"]).reset_index(drop=True)\n",
    "                    \n",
    "                    if len(df_batch) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Feature engineering\n",
    "                    try:\n",
    "                        df_batch[\"composition\"] = df_batch[\"structure\"].apply(lambda s: s.composition)\n",
    "                        \n",
    "                        # Element features\n",
    "                        elem_features = []\n",
    "                        for comp in tqdm(df_batch[\"composition\"], desc=\"Element features\", leave=False):\n",
    "                            try:\n",
    "                                elem_features.append(elem_feat.featurize(comp))\n",
    "                            except Exception as e:\n",
    "                                if not TESTING:  # Only print errors in full mode\n",
    "                                    print(f\"Error in element featurization: {e}\")\n",
    "                                elem_features.append([np.nan] * len(elem_feat.feature_labels()))\n",
    "                        \n",
    "                        df_elem = pd.DataFrame(elem_features, columns=elem_feat.feature_labels())\n",
    "                        \n",
    "                        # Density features\n",
    "                        dens_features = []\n",
    "                        for struct in tqdm(df_batch[\"structure\"], desc=\"Density features\", leave=False):\n",
    "                            try:\n",
    "                                dens_features.append(dens_feat.featurize(struct))\n",
    "                            except Exception as e:\n",
    "                                if not TESTING:  # Only print errors in full mode\n",
    "                                    print(f\"Error in density featurization: {e}\")\n",
    "                                dens_features.append([np.nan] * len(dens_feat.feature_labels()))\n",
    "                        \n",
    "                        df_dens = pd.DataFrame(dens_features, columns=dens_feat.feature_labels())\n",
    "                        \n",
    "                        # Symmetry features\n",
    "                        sym_features = []\n",
    "                        for struct in tqdm(df_batch[\"structure\"], desc=\"Symmetry features\", leave=False):\n",
    "                            try:\n",
    "                                sym_features.append(sym_feat.featurize(struct))\n",
    "                            except Exception as e:\n",
    "                                if not TESTING:  # Only print errors in full mode\n",
    "                                    print(f\"Error in symmetry featurization: {e}\")\n",
    "                                sym_features.append([np.nan] * len(sym_feat.feature_labels()))\n",
    "                        \n",
    "                        df_sym = pd.DataFrame(sym_features, columns=sym_feat.feature_labels())\n",
    "                        \n",
    "                        # Combine all features\n",
    "                        df_feat = pd.concat([\n",
    "                            df_batch.reset_index(drop=True), \n",
    "                            df_elem.reset_index(drop=True), \n",
    "                            df_dens.reset_index(drop=True), \n",
    "                            df_sym.reset_index(drop=True)\n",
    "                        ], axis=1)\n",
    "                        \n",
    "                        dfs.append(df_feat)\n",
    "                        fetched += len(df_feat)\n",
    "                        \n",
    "                        print(f\"Processed batch {batch_count + 1}, fetched {fetched}/{N_TOTAL} entries\")\n",
    "                        \n",
    "                        if fetched >= N_TOTAL:\n",
    "                            break\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing batch {batch_count}: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    batch_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in API call: {e}\")\n",
    "                return None\n",
    "        \n",
    "        if not dfs:\n",
    "            print(\"No data was successfully fetched!\")\n",
    "            return None\n",
    "        \n",
    "        # Combine all batches\n",
    "        df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "        \n",
    "        # Ensure we don't exceed N_TOTAL\n",
    "        if len(df) > N_TOTAL:\n",
    "            df = df.iloc[:N_TOTAL].copy()\n",
    "        \n",
    "        print(f\"Total entries fetched: {len(df)}\")\n",
    "        \n",
    "        # Save to cache\n",
    "        with open(CACHE_FILE, 'wb') as f:\n",
    "            pickle.dump(df, f)\n",
    "        print(f\"Data saved to cache: {CACHE_FILE}\")\n",
    "    \n",
    "    # Fe / non-Fe split\n",
    "    def is_fe(x):\n",
    "        if isinstance(x, list):\n",
    "            return 'Fe' in x\n",
    "        try:\n",
    "            if isinstance(x, str):\n",
    "                return 'Fe' in eval(x)\n",
    "            return 'Fe' in x\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    fe_mask = df['elements'].apply(is_fe)\n",
    "    df_fe = df[fe_mask].reset_index(drop=True)\n",
    "    df_non_fe = df[~fe_mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Non-Fe materials: {len(df_non_fe)}\")\n",
    "    print(f\"Fe-containing materials: {len(df_fe)}\")\n",
    "    \n",
    "    # In testing mode, ensure we have enough data for both splits\n",
    "    if TESTING:\n",
    "        if len(df_non_fe) < 50:\n",
    "            print(\"Warning: Very few non-Fe materials for training in testing mode\")\n",
    "        if len(df_fe) < 10:\n",
    "            print(\"Warning: Very few Fe materials for testing in testing mode\")\n",
    "    \n",
    "    return {'train_data': df_non_fe, 'test_data': df_fe, 'full_data': df}\n",
    "\n",
    "# N1: Impute Node\n",
    "def impute_mean(data, **params):\n",
    "    imp = SimpleImputer(strategy='mean')\n",
    "    X_train = imp.fit_transform(data['X_train'])\n",
    "    X_val = imp.transform(data['X_val'])\n",
    "    X_test = imp.transform(data['X_test']) if data['X_test'] is not None else None\n",
    "    return {'X_train': X_train, 'X_val': X_val, 'X_test': X_test, 'imputer': imp}\n",
    "\n",
    "def impute_knn(data, n_neighbors=5, **params):\n",
    "    # In testing mode, use fewer neighbors if needed\n",
    "    if TESTING and data['X_train'].shape[0] < n_neighbors:\n",
    "        n_neighbors = max(1, data['X_train'].shape[0] - 1)\n",
    "    \n",
    "    imp = KNNImputer(n_neighbors=n_neighbors)\n",
    "    X_train = imp.fit_transform(data['X_train'])\n",
    "    X_val = imp.transform(data['X_val'])\n",
    "    X_test = imp.transform(data['X_test']) if data['X_test'] is not None else None\n",
    "    return {'X_train': X_train, 'X_val': X_val, 'X_test': X_test, 'imputer': imp}\n",
    "\n",
    "# N2: 特征矩阵生成\n",
    "def feature_matrix(data):\n",
    "    train_df = data['train_data']\n",
    "    test_df = data['test_data']\n",
    "    \n",
    "    # Drop columns with missing rate > threshold\n",
    "    missing_threshold = 0.8 if TESTING else 0.5  # More lenient in testing mode\n",
    "    cols_drop = [c for c in train_df.columns if train_df[c].isna().mean() > missing_threshold]\n",
    "    print(f\"Dropping {len(cols_drop)} columns with >{missing_threshold*100}% missing values\")\n",
    "    \n",
    "    # Select feature columns\n",
    "    feat_cols = [c for c in train_df.columns \n",
    "                 if c not in ['material_id', 'structure', 'elements', 'formula_pretty', 'composition', TARGET_PROP] \n",
    "                 and c not in cols_drop]\n",
    "    \n",
    "    print(f\"Using {len(feat_cols)} feature columns\")\n",
    "    \n",
    "    # Convert to numeric\n",
    "    X_train_full = train_df[feat_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    X_test_full = test_df[feat_cols].apply(pd.to_numeric, errors='coerce') if len(test_df) > 0 else None\n",
    "    \n",
    "    # Train/validation split\n",
    "    split_ratio = 0.8\n",
    "    split_idx = int(split_ratio * len(X_train_full))\n",
    "    \n",
    "    # Ensure minimum sizes in testing mode\n",
    "    if TESTING:\n",
    "        if split_idx < 10:\n",
    "            split_idx = max(10, len(X_train_full) - 5)\n",
    "    \n",
    "    X_train, X_val = X_train_full.iloc[:split_idx], X_train_full.iloc[split_idx:]\n",
    "    X_test = X_test_full\n",
    "    \n",
    "    print(f\"Train/Val/Test split: {len(X_train)}/{len(X_val)}/{len(X_test) if X_test is not None else 0}\")\n",
    "    \n",
    "    return {'X_train': X_train, 'X_val': X_val, 'X_test': X_test, 'feature_names': feat_cols}\n",
    "\n",
    "# N3: Feature Selection\n",
    "def no_selection(data, **params):\n",
    "    return data\n",
    "\n",
    "def variance_selection(data, var_ratio=0.01, **params):\n",
    "    # In testing mode, use more lenient variance threshold\n",
    "    if TESTING:\n",
    "        var_ratio = 0.001\n",
    "    \n",
    "    selector = VarianceThreshold(threshold=var_ratio)\n",
    "    X_train = selector.fit_transform(data['X_train'])\n",
    "    X_val = selector.transform(data['X_val'])\n",
    "    X_test = selector.transform(data['X_test']) if data['X_test'] is not None else None\n",
    "    \n",
    "    print(f\"Variance selection: {data['X_train'].shape[1]} -> {X_train.shape[1]} features\")\n",
    "    \n",
    "    return {'X_train': X_train, 'X_val': X_val, 'X_test': X_test, 'selector': selector}\n",
    "\n",
    "# N4: Scaling\n",
    "def scale_standard(data, **params):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(data['X_train'])\n",
    "    X_val = scaler.transform(data['X_val'])\n",
    "    X_test = scaler.transform(data['X_test']) if data['X_test'] is not None else None\n",
    "    return {'X_train': X_train, 'X_val': X_val, 'X_test': X_test, 'scaler': scaler}\n",
    "\n",
    "def scale_robust(data, **params):\n",
    "    scaler = RobustScaler()\n",
    "    X_train = scaler.fit_transform(data['X_train'])\n",
    "    X_val = scaler.transform(data['X_val'])\n",
    "    X_test = scaler.transform(data['X_test']) if data['X_test'] is not None else None\n",
    "    return {'X_train': X_train, 'X_val': X_val, 'X_test': X_test, 'scaler': scaler}\n",
    "\n",
    "# N5: Learner Node\n",
    "def train_rf(data, n_estimators=100, max_depth=None, **params):\n",
    "    # Adjust parameters for testing mode\n",
    "    if TESTING:\n",
    "        n_estimators = 20  # Fewer trees for faster training\n",
    "        max_depth = 5      # Shallow trees to prevent overfitting with small data\n",
    "    \n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators, \n",
    "        max_depth=max_depth, \n",
    "        random_state=42, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(data['X_train'], data['y_train'])\n",
    "    y_val_pred = model.predict(data['X_val'])\n",
    "    y_test_pred = model.predict(data['X_test']) if data['X_test'] is not None else None\n",
    "    return {'model': model, 'y_val_pred': y_val_pred, 'y_test_pred': y_test_pred}\n",
    "\n",
    "def train_gbr(data, n_estimators=100, learning_rate=0.1, max_depth=3, **params):\n",
    "    # Adjust parameters for testing mode\n",
    "    if TESTING:\n",
    "        n_estimators = 20     # Fewer estimators\n",
    "        learning_rate = 0.2   # Higher learning rate for faster convergence\n",
    "        max_depth = 3         # Keep shallow\n",
    "    \n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=n_estimators, \n",
    "        learning_rate=learning_rate, \n",
    "        max_depth=max_depth, \n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(data['X_train'], data['y_train'])\n",
    "    y_val_pred = model.predict(data['X_val'])\n",
    "    y_test_pred = model.predict(data['X_test']) if data['X_test'] is not None else None\n",
    "    return {'model': model, 'y_val_pred': y_val_pred, 'y_test_pred': y_test_pred}\n",
    "\n",
    "# ========= Assemble nodes =========\n",
    "nodes = {\n",
    "    'N0': Node('DataFetch', {'api': fetch_and_featurize}),\n",
    "    'N1': Node('Impute', {'mean': impute_mean, 'knn': impute_knn}),\n",
    "    'N2': Node('FeatureMatrix', {'default': feature_matrix}),\n",
    "    'N3': Node('FeatSelect', {'none': no_selection, 'variance': variance_selection}),\n",
    "    'N4': Node('Scale', {'std': scale_standard, 'robust': scale_robust}),\n",
    "    'N5': Node('Learner', {'rf': train_rf, 'gbr': train_gbr}),\n",
    "}\n",
    "\n",
    "# ========= 主 pipeline =========\n",
    "def run_pipeline(config=None):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Node0: fetch + featurize\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Starting ML Pipeline ({'TESTING' if TESTING else 'FULL'} mode)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    n0_out = nodes['N0'].execute('api', {}, None)\n",
    "    if n0_out is None:\n",
    "        print(\"Failed to fetch data!\")\n",
    "        return None\n",
    "    \n",
    "    train_df = n0_out['train_data']\n",
    "    test_df = n0_out['test_data']\n",
    "    \n",
    "    if len(train_df) == 0:\n",
    "        print(\"No training data available!\")\n",
    "        return None\n",
    "    \n",
    "    # Check minimum data requirements\n",
    "    if TESTING and len(train_df) < 20:\n",
    "        print(f\"Warning: Only {len(train_df)} training samples in testing mode\")\n",
    "    \n",
    "    # Node2: 特征矩阵\n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"Feature Matrix Generation\")\n",
    "    print(\"=\" * 30)\n",
    "    n2_out = nodes['N2'].execute('default', {}, {'train_data': train_df, 'test_data': test_df})\n",
    "    \n",
    "    # y split\n",
    "    y_train_full = train_df[TARGET_PROP].values\n",
    "    y_test_full = test_df[TARGET_PROP].values if len(test_df) > 0 else None\n",
    "    \n",
    "    split_ratio = 0.8\n",
    "    split_idx = int(split_ratio * len(y_train_full))\n",
    "    \n",
    "    # Ensure minimum sizes in testing mode\n",
    "    if TESTING:\n",
    "        if split_idx < 10:\n",
    "            split_idx = max(10, len(y_train_full) - 5)\n",
    "    \n",
    "    y_train, y_val = y_train_full[:split_idx], y_train_full[split_idx:]\n",
    "    \n",
    "    print(f\"Training samples: {len(y_train)}\")\n",
    "    print(f\"Validation samples: {len(y_val)}\")\n",
    "    if y_test_full is not None:\n",
    "        print(f\"Test samples: {len(y_test_full)}\")\n",
    "    \n",
    "    # Node1: impute\n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"Imputation\")\n",
    "    print(\"=\" * 30)\n",
    "    n1_out = nodes['N1'].execute('mean', {}, {\n",
    "        'X_train': n2_out['X_train'], \n",
    "        'X_val': n2_out['X_val'], \n",
    "        'X_test': n2_out['X_test']\n",
    "    })\n",
    "    \n",
    "    # Node3: feature selection\n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"Feature Selection\")\n",
    "    print(\"=\" * 30)\n",
    "    selection_method = 'none' if TESTING else 'none'  # Keep simple for testing\n",
    "    n3_out = nodes['N3'].execute(selection_method, {}, n1_out)\n",
    "    \n",
    "    # Node4: scaling\n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"Scaling\")\n",
    "    print(\"=\" * 30)\n",
    "    n4_out = nodes['N4'].execute('std', {}, n3_out)\n",
    "    \n",
    "    # Node5: learner\n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"Model Training\")\n",
    "    print(\"=\" * 30)\n",
    "    n4_out['y_train'] = y_train\n",
    "    n4_out['y_val'] = y_val\n",
    "    n4_out['y_test'] = y_test_full\n",
    "    \n",
    "    # Use RF for both modes, but with different params\n",
    "    n5_out = nodes['N5'].execute('rf', {}, n4_out)\n",
    "    \n",
    "    # Metrics\n",
    "    mae_val = mean_absolute_error(y_val, n5_out['y_val_pred'])\n",
    "    r2_val = r2_score(y_val, n5_out['y_val_pred'])\n",
    "    \n",
    "    if y_test_full is not None and n5_out['y_test_pred'] is not None:\n",
    "        mae_test = mean_absolute_error(y_test_full, n5_out['y_test_pred'])\n",
    "        r2_test = r2_score(y_test_full, n5_out['y_test_pred'])\n",
    "    else:\n",
    "        mae_test, r2_test = None, None\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"Results\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"Validation MAE: {mae_val:.3f}\")\n",
    "    print(f\"Validation R²: {r2_val:.3f}\")\n",
    "    \n",
    "    if mae_test is not None:\n",
    "        print(f\"Test MAE: {mae_test:.3f}\")\n",
    "        print(f\"Test R²: {r2_test:.3f}\")\n",
    "    \n",
    "    print(f\"\\nTotal pipeline time: {time.time() - start:.1f}s\")\n",
    "    \n",
    "    return {\n",
    "        'mae_val': mae_val, \n",
    "        'r2_val': r2_val, \n",
    "        'mae_test': mae_test, \n",
    "        'r2_test': r2_test, \n",
    "        'model': n5_out['model'],\n",
    "        'train_size': len(y_train),\n",
    "        'val_size': len(y_val),\n",
    "        'test_size': len(y_test_full) if y_test_full is not None else 0,\n",
    "        'mode': 'TESTING' if TESTING else 'FULL'\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Running pipeline in {'TESTING' if TESTING else 'FULL'} mode...\")\n",
    "    print(f\"Target samples: {N_TOTAL}\")\n",
    "    print(f\"Cache file: {CACHE_FILE}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = run_pipeline()\n",
    "    if results:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Mode: {results['mode']}\")\n",
    "        print(f\"Data sizes - Train: {results['train_size']}, Val: {results['val_size']}, Test: {results['test_size']}\")\n",
    "        print(f\"Performance - Val MAE: {results['mae_val']:.3f}, Val R²: {results['r2_val']:.3f}\")\n",
    "        if results['mae_test'] is not None:\n",
    "            print(f\"Test performance - MAE: {results['mae_test']:.3f}, R²: {results['r2_test']:.3f}\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"PIPELINE FAILED!\")\n",
    "        print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b883b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline started...\n",
      "[N0] Fetching + feature engineering from MP API (20000 entries)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48103\\AppData\\Roaming\\Python\\Python38\\site-packages\\mp_api\\client\\mprester.py:193: UserWarning: mpcontribs-client not installed. Install the package to query MPContribs data, or construct pourbaix diagrams: 'pip install mpcontribs-client'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39de939602574011acf4051fe0a734fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving SummaryDoc documents:   0%|          | 0/21000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|▍                                      | 245/21000 [00:03<05:13, 66.22it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for ** or pow(): 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ebae8e645d3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline started...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done. Main metrics:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'metrics'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ebae8e645d3d>\u001b[0m in \u001b[0;36mrun_pipeline\u001b[1;34m(config, save_dash_test, verbose, rl_api, return_intermediates)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m     \u001b[0mn0_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'N0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'api'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn0_out\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Failed to fetch data!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ebae8e645d3d>\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, method, params, data)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethods\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethods\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# -- N0: 数据抓取+特征提取 --\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ebae8e645d3d>\u001b[0m in \u001b[0;36mfetch_and_featurize\u001b[1;34m(data, cache)\u001b[0m\n\u001b[0;32m     96\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"composition\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 ], columns=elem_feat.feature_labels())\n\u001b[1;32m---> 98\u001b[1;33m                 df_dens = pd.DataFrame([\n\u001b[0m\u001b[0;32m     99\u001b[0m                     \u001b[0mdens_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeaturize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdens_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"structure\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ebae8e645d3d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     97\u001b[0m                 ], columns=elem_feat.feature_labels())\n\u001b[0;32m     98\u001b[0m                 df_dens = pd.DataFrame([\n\u001b[1;32m---> 99\u001b[1;33m                     \u001b[0mdens_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeaturize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdens_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"structure\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 ], columns=dens_feat.feature_labels())\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\matminer\\featurizers\\structure\\order.py\u001b[0m in \u001b[0;36mfeaturize\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mtotal_rad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0msite\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mtotal_rad\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecie\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matomic_radius\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtotal_rad\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvolume\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for ** or pow(): 'NoneType' and 'int'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef44ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
